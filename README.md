# ğŸ“ AcademIQ â€” Academic Research Paper Assistant

> *An intelligent RAG-powered system to query, summarize, and generate questions from academic research papers.*
---

## ğŸ“Œ What is AcademIQ?

AcademIQ is a **Retrieval-Augmented Generation (RAG)** application that lets you interact intelligently with academic PDF papers. Instead of relying on an AI's pre-trained knowledge, AcademIQ forces the model to answer **only from the actual content of your uploaded paper** â€” making every response accurate, grounded, and traceable to specific pages.

You can:
- Ask any question about a research paper and get precise, cited answers
- Select from multiple papers without reprocessing them every time
- Auto-generate MCQ, Short Answer, or Long Answer questions for study or evaluation
- Track your question history across the session

This makes AcademIQ useful for **students, researchers, educators, and academics** who work with large volumes of technical literature.

---

## âœ¨ Features

| Feature | Description |
|---|---|
| ğŸ“‚ **Multi-PDF Support** | Load any PDF from `data/samples/` via a dropdown selector |
| ğŸ” **Semantic Search** | Finds the most relevant sections of a paper using vector similarity |
| ğŸ¤– **RAG Pipeline** | Answers are grounded strictly in the paper's content, not AI guesswork |
| ğŸ“ **Question Generation** | Auto-generates MCQ, Short Answer, and Long Answer questions |
| ğŸ—‚ï¸ **Per-PDF Vectorstores** | Each paper gets its own FAISS index â€” built once, loaded instantly after |
| ğŸ“„ **Source Page Citations** | Every answer shows which pages of the paper it was drawn from |
| ğŸ•‘ **Chat History** | Last 5 questions are saved in the sidebar for easy reference |
| ğŸ¨ **Academic UI** | Professional academic interface built with custom CSS in Streamlit |
| ğŸ’» **CPU Optimised** | No GPU required â€” runs on any standard machine |
| ğŸ” **Secure API Handling** | API keys stored in `.env`, never in source code |

---

## ğŸ—ï¸ Project Architecture

```
PDF File
   â”‚
   â–¼
PyPDFLoader â”€â”€â–º Raw Text Pages
   â”‚
   â–¼
RecursiveCharacterTextSplitter â”€â”€â–º ~1000 character chunks (200 overlap)
   â”‚
   â–¼
HuggingFace Embeddings (all-MiniLM-L6-v2) â”€â”€â–º 384-dimensional vectors
   â”‚
   â–¼
FAISS Vector Store â”€â”€â–º Saved to disk per PDF (vectorstore/<pdf-name>/)
        â”‚
        â”‚  At query time:
        â–¼
User Question â”€â”€â–º Embed â”€â”€â–º Similarity Search â”€â”€â–º Top 4 Chunks
                                                        â”‚
                                                        â–¼
                                           Groq LLM (LLaMA 3.1 8B Instant)
                                           + Strict Context Prompt
                                                        â”‚
                                                        â–¼
                                             Answer + Source Page Numbers
```

### How Each Component Works

**PDF Loader** uses `PyPDFLoader` from LangChain Community to extract raw text from every page of the document, preserving page metadata for source citation.

**Text Splitter** uses `RecursiveCharacterTextSplitter` with a chunk size of 1000 characters and 200-character overlap. The overlap ensures important content at chunk boundaries is never lost.

**Embeddings** uses HuggingFace's `sentence-transformers/all-MiniLM-L6-v2` model to convert each text chunk into a 384-dimensional vector that captures its semantic meaning â€” not just keywords.

**FAISS Vector Store** stores all chunk vectors in a Facebook AI Similarity Search index saved locally to disk. Each PDF gets its own dedicated index so it's only built once.

**Retriever** takes the user's question, embeds it with the same model, and performs a cosine similarity search to retrieve the 4 most relevant chunks from the paper.

**LLM (Groq + LLaMA 3.1)** receives the 4 retrieved chunks as context alongside a strict prompt instructing it to answer only from the provided context. Groq is used as the inference provider for its free tier and extremely fast response speeds.

**Question Generator** uses the same RAG chain but with a structured JSON prompt, instructing the LLM to output either MCQs (with 4 options and correct answer), Short Answers (2-3 sentences), or Long Answers (full paragraph), which are then parsed and rendered as styled cards.

---

## ğŸ“ Project Structure

```
academic-rag-summarizer/
â”‚
â”œâ”€â”€ app.py                  # Streamlit web app (UI, tabs, session state)
â”œâ”€â”€ rag_pipeline.py         # RAG logic, vectorstore management, question generation
â”œâ”€â”€ ingest.py               # Standalone PDF ingestion script (optional)
â”œâ”€â”€ requirements.txt        # All Python dependencies
â”œâ”€â”€ .env                    # API keys â€” never committed to Git
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ samples/            # Place your academic PDFs here
â”‚
â”œâ”€â”€ vectorstore/            # Auto-created â€” one subfolder per PDF
â”‚   â””â”€â”€ <pdf-name>/
â”‚       â”œâ”€â”€ index.faiss
â”‚       â””â”€â”€ index.pkl
â”‚
â””â”€â”€ venv/                   # Virtual environment â€” not committed
```

---

## âš™ï¸ Technologies Used

| Technology | Version | Purpose |
|---|---|---|
| Python | 3.12 | Core language |
| LangChain | 0.3.25 | RAG orchestration framework |
| LangChain Community | 0.3.24 | PDF loader, FAISS integration |
| LangChain Core | 0.3.63 | Prompts, chains, base abstractions |
| LangChain Groq | 0.2.3 | Groq LLM integration |
| LangChain HuggingFace | 0.1.2 | Embedding model integration |
| LangChain Text Splitters | 0.3.8 | Document chunking |
| FAISS CPU | â€” | Vector similarity search |
| Sentence Transformers | â€” | `all-MiniLM-L6-v2` embedding model |
| Groq API | â€” | LLaMA 3.1 8B Instant inference |
| Streamlit | â€” | Web UI framework |
| PyPDF | â€” | PDF text extraction |
| Python-dotenv | â€” | Environment variable management |

---

## ğŸ› ï¸ Installation & Setup

### âœ… Step 1 â€” Clone the Repository

```bash
git clone https://github.com/SOMESHSINGH25/academic-rag-summarizer.git
cd academic-rag-summarizer
```

### âœ… Step 2 â€” Create Virtual Environment

```bash
python -m venv venv

# Windows
venv\Scripts\activate

# macOS / Linux
source venv/bin/activate
```

### âœ… Step 3 â€” Install Dependencies

```bash
pip install langchain==0.3.25 langchain-community==0.3.24 langchain-core==0.3.63 langchain-huggingface==0.1.2 langchain-text-splitters==0.3.8 langchain-groq==0.2.3
pip install faiss-cpu sentence-transformers streamlit pypdf python-dotenv torch
```

> âš ï¸ **Important:** Install all LangChain packages together in a single command to ensure pip resolves compatible versions. Installing them separately can cause version conflicts.

### âœ… Step 4 â€” Get a Groq API Key (Free)

1. Sign up at [https://console.groq.com](https://console.groq.com)
2. Go to **API Keys** and create a new key
3. Copy the key â€” it starts with `gsk_...`

### âœ… Step 5 â€” Configure Environment Variables

Create a `.env` file in the root of the project:

```
GROQ_API_KEY=gsk_your_api_key_here
```

Rules for the `.env` file:
- No quotes around the key
- No spaces around the `=`
- File named exactly `.env` (not `env.txt` or `.env.txt`)

### âœ… Step 6 â€” Add Your PDFs

Place your academic papers inside the samples folder:

```
data/samples/paper1.pdf
data/samples/paper2.pdf
```

---

## â–¶ï¸ Running the Application

```bash
streamlit run app.py
```

The app will open automatically in your browser at `http://localhost:8501`.

> **No need to run `ingest.py` manually.** The app automatically builds the FAISS vectorstore the first time you load a PDF, and loads it instantly from disk on subsequent uses.

---

## ğŸ–¥ï¸ How to Use

### Asking Questions

1. Select a paper from the **sidebar dropdown**
2. Click **"Load This Paper"** â€” wait for the success message
3. Go to the **"Ask Questions"** tab
4. Type your question and click **"Ask â†’"**
5. The answer appears with **source page citations** below it
6. Previous questions are saved in the sidebar and in expandable cards

Example questions:
- *What is the main contribution of this paper?*
- *What datasets were used in the experiments?*
- *What are the limitations mentioned by the authors?*
- *How does the proposed method compare to baselines?*

### Generating Questions

1. Load a paper using the sidebar
2. Go to the **"Generate Questions"** tab
3. Choose the question type: **MCQ**, **Short Answer**, or **Long Answer**
4. Select the number of questions (3, 5, 7, or 10)
5. Optionally add a topic focus (e.g. *"methodology"*, *"results"*)
6. Click **"Generate Questions â†’"**

MCQs are displayed with 4 options (Aâ€“D) and the correct answer highlighted in green. Short Answers show a concise 2-3 sentence response. Long Answers are shown in expandable cards with full paragraph explanations.

---

## ğŸŒŸ Key Design Decisions

**Per-PDF Vectorstores** â€” Rather than a single shared index, each PDF gets its own FAISS index stored under `vectorstore/<pdf-name>/`. This means switching between papers is instant after the first load, and adding new papers doesn't require re-ingesting existing ones.

**Strict Context Prompting** â€” The LLM prompt explicitly instructs the model to answer *only* from the provided context. This is the core of what makes RAG reliable â€” it prevents hallucinations and keeps answers faithful to the actual paper.

**Groq over OpenAI** â€” Groq provides free, fast inference for open-source models like LLaMA 3.1, making this project fully free to run with no credit card required.

**LangChain Version Pinning** â€” All LangChain packages are pinned to the `0.3.x` family. The `1.x` releases introduced breaking API changes. Installing packages individually with pip can silently upgrade `langchain-core` to an incompatible version, so all packages are installed together.

---

## ğŸ” Security

- API keys are stored in `.env` and never committed to Git
- `.gitignore` prevents `.env` from being pushed to GitHub
- No credentials are hardcoded anywhere in source files
- The vectorstore uses `allow_dangerous_deserialization=True` only for locally built, trusted FAISS indexes

---

## ğŸ§ª Future Improvements

- [ ] Upload PDFs directly from the UI without manual file copying
- [ ] Multi-document comparison across papers
- [ ] Automatic paper summarization on load
- [ ] Citation generation in APA / MLA format
- [ ] Fine-tuned academic domain LLM
- [ ] Cloud deployment (Streamlit Community Cloud / Hugging Face Spaces)
- [ ] Multi-language paper support
- [ ] Export questions as PDF or Word document

---

## ğŸ™ Acknowledgements

- **Intel Unnati & GenAI4GenZ Hackathon** â€” for the opportunity and motivation to build this
- **HuggingFace** â€” for the `all-MiniLM-L6-v2` embedding model
- **LangChain** â€” for the RAG orchestration framework
- **Groq** â€” for free and fast LLaMA inference
- **Meta AI** â€” for the open-source LLaMA 3.1 model
- **Facebook AI Research** â€” for the FAISS library

---

## ğŸ‘¤ Author

**Somesh Singh**

**B.Tech Information Technology**

**Bharati Vidyapeeth (Deemed To Be University), College of Engineering, Pune**

---

## ğŸ“œ License

This project is developed for educational and hackathon purposes.
